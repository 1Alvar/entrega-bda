{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cd995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Reviews_Amazon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab338bf",
   "metadata": {},
   "source": [
    "## 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2a173b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0635f",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5998593e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(_c7='1350345600', count=1140)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"_c7\").count().orderBy(\"count\", ascending=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761487a",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d23f72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c1='B007JFMH8M', count=913),\n",
       " Row(_c1='B002QWP89S', count=632),\n",
       " Row(_c1='B0026RQTGE', count=632),\n",
       " Row(_c1='B002QWHJOU', count=632),\n",
       " Row(_c1='B002QWP8H0', count=632),\n",
       " Row(_c1='B003B3OOPA', count=623),\n",
       " Row(_c1='B001EO5Q64', count=567),\n",
       " Row(_c1='B0013NUGDE', count=564),\n",
       " Row(_c1='B007M832YY', count=564),\n",
       " Row(_c1='B0026KNQSA', count=564)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"_c1\").count().orderBy(\"count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b96c68",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f69c60e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+----------+---+---+---+----------+--------------------+--------------------+\n",
      "|_c0|       _c1|           _c2|       _c3|_c4|_c5|_c6|       _c7|                 _c8|                 _c9|\n",
      "+---+----------+--------------+----------+---+---+---+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|delmartian|  1|  1|  5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "+---+----------+--------------+----------+---+---+---+----------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"_c3\"] == \"delmartian\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f2f33",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3b40e26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='15', _c1='B001GVISJM', _c2='A2MUGFV2TDQ47K', _c3='\"Lynrie \"\"Oh HELL no\"\"\"', _c4='4', _c5='5', _c6='5', _c7='1268352000', _c8='Strawberry Twizzlers - Yummy', _c9='The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.'),\n",
       " Row(_c0='16', _c1='B001GVISJM', _c2='A1CZX3CP8IKQIJ', _c3='Brian A. Lee', _c4='4', _c5='5', _c6='5', _c7='1262044800', _c8='Lots of twizzlers, just what you expect.', _c9=\"My daughter loves twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry twizzlers.\"),\n",
       " Row(_c0='84', _c1='B0019CW0HE', _c2='A1FD9E5C06UB6B', _c3='BRENDA DEMERS', _c4='5', _c5='5', _c6='3', _c7='1301011200', _c8='Natural Balance Lamb and Rice', _c9='While my dogs like all of the flavors that we have tried of this dog food, for some reason their itching increased when I tried the lamb and rice. I have some very itchy dogs and am giving them a limited ingredient dog food to try to help. The duck and sweet potato cut down on the itching significantly, but when we tried lamb and rice they started itching more once again. I like Natural Balance for the quality ingredients.'),\n",
       " Row(_c0='101', _c1='B004K2IHUO', _c2='A1SYSKR79LA2CB', _c3='\"Mycroft \"\"Virture is its own punishment\"\"\"', _c4='4', _c5='5', _c6='5', _c7='1233014400', _c8='Taste wise it is a 6 star item', _c9='\"The mouth says, \"\"How do I love thee'),\n",
       " Row(_c0='119', _c1='B003SE19UK', _c2='AD1WWGMEQ8FWW', _c3='Bob Rex', _c4='5', _c5='5', _c6='5', _c7='1290988800', _c8='So Far So Good', _c9=\"I had heard a little about this product from the local pet store, then tried a small bag for our 2 cats of about 3 and 6 years respectively. The female, younger one took to it right away, and the older male, who was unfortunately weaned on the junk cat food with all the corn meal and etc... in it, and would usually refuse a diet change, actually liked it too. The older male is overweight and we were trying to switch him to soft food on the doctor's request, but he totally refused it. He is a posh cat after all. He still consumes too many calories, the fatty, but at least I know he's eating good food. Of course, I know this product seems to cost more than most other feline food on the market, but I just can't see feeding our animals the lousy alternatives that are out there, marketed as being healthy. Why do commercials show cats eating a vegetarian diet which is mostly corn meal? Is that right? I'll stay with this for now, but stay tuned.<br /><br />I'm updating this review after 9 months of using this cat food for our two cats. I upgraded the stars from 4 to 5 as it really has been a good food for the two of them. Garfield still eats a little too much, but he's just a big cat who doesn't exercise much, let's face it. Our female cat will be 4 years old in a couple months and she's so healthy. She's tiny, but happy and agile. I tried the chicken version of Holistic Select, but Garfield seemd to miss this sardine version so we went back to this one. I've ordered the duck version and will update this once they try it.\")]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df[\"_c5\"] == \"5\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e08dba",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fe468de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c3='Heather Dube', count=1), Row(_c3='Monie', count=1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"_c3\").count().orderBy(\"count\", ascending=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6beda",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd171803",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'sum' given input columns: [_c6];\n'Sort ['sum DESC NULLS LAST], true\n+- Aggregate [_c6#109], [_c6#109]\n   +- Relation [_c0#103,_c1#104,_c2#105,_c3#106,_c4#107,_c5#108,_c6#109,_c7#110,_c8#111,_c9#112] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_c6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfirst()\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/sql/dataframe.py:1421\u001b[0m, in \u001b[0;36mDataFrame.sort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;124;03m    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1421\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sort_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/share/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'sum' given input columns: [_c6];\n'Sort ['sum DESC NULLS LAST], true\n+- Aggregate [_c6#109], [_c6#109]\n   +- Relation [_c0#103,_c1#104,_c2#105,_c3#106,_c4#107,_c5#108,_c6#109,_c7#110,_c8#111,_c9#112] csv\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"_c6\").sum().orderBy(\"sum\", ascending=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba22f6f",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecdcadcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "|_c0|       _c1|           _c2|                 _c3|_c4|_c5|_c6|       _c7|                 _c8|                 _c9|\n",
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|  1|  1|  5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|  0|  0|  5|1350777600|         Great taffy|Great taffy at a ...|\n",
      "|  7|B006K2ZZ7K|A1SP2KVKFXXRU1|   David C. Sullivan|  0|  0|  5|1340150400|Great!  Just as g...|This saltwater ta...|\n",
      "|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|  0|  0|  5|1336003200|Wonderful, tasty ...|This taffy is so ...|\n",
      "|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|  1|  1|  5|1322006400|          Yay Barley|Right now I'm mos...|\n",
      "| 10|B00171APVA|A21BT40VZCCYT4|       Carol A. Reed|  0|  0|  5|1351209600|    Healthy Dog Food|This is a very he...|\n",
      "| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|  1|  1|  5|1107820800|The Best Hot Sauc...|I don't know if i...|\n",
      "| 12|B0009XLVG0|A2725IB4YY9JEB|\"A Poeng \"\"Sparky...|  4|  4|  5|1282867200|\"My cats LOVE thi...|One of my boys ne...|\n",
      "| 15|B001GVISJM|A2MUGFV2TDQ47K|\"Lynrie \"\"Oh HELL...|  4|  5|  5|1268352000|Strawberry Twizzl...|The Strawberry Tw...|\n",
      "| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|  4|  5|  5|1262044800|Lots of twizzlers...|My daughter loves...|\n",
      "| 18|B001GVISJM| AFKW14U97Z6QO|               Becca|  0|  0|  5|1345075200|            Love it!|I am very satisfi...|\n",
      "| 19|B001GVISJM|A2A9X58G2GTBLP|             Wolfee1|  0|  0|  5|1324598400|  GREAT SWEET CANDY!|Twizzlers, Strawb...|\n",
      "| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|  0|  0|  5|1318032000|Home delivered tw...|Candy was deliver...|\n",
      "| 21|B001GVISJM|A1WO0KGLPR5PV6|            mom2emma|  0|  0|  5|1313452800|        Always fresh|My husband is a T...|\n",
      "| 22|B001GVISJM| AZOF9E17RGZH8|      Tammy Anderson|  0|  0|  5|1308960000|           TWIZZLERS|\"I bought these f...|\n",
      "| 23|B001GVISJM| ARYVQL4N737A1|       Charles Brown|  0|  0|  5|1304899200|  Delicious product!|I can remember bu...|\n",
      "| 24|B001GVISJM| AJ613OLZZUG7V|              Mare's|  0|  0|  5|1304467200|           Twizzlers|I love this candy...|\n",
      "| 25|B001GVISJM|A22P2J09NJ9HKE|\"S. Cabanaugh \"\"j...|  0|  0|  5|1295481600|Please sell these...|I have lived out ...|\n",
      "| 26|B001GVISJM|A3FONPR03H3PJS|\"Deborah S. Linze...|  0|  0|  5|1288310400|Twizzlers - Straw...|\"Product received...|\n",
      "| 29|B00144C10S|A2F4LZVGFLD1OB|              DaisyH|  0|  0|  5|1338854400|              YUMMY!|I got this for my...|\n",
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"_c6\"] == \"5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4c3aafe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "|_c0|       _c1|           _c2|                 _c3|_c4|_c5|_c6|       _c7|                 _c8|                 _c9|\n",
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|  0|  0|  1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|  1|  1|  4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|  3|  3|  2|1307923200|      Cough Medicine|If you are lookin...|\n",
      "|  6|B006K2ZZ7K| ADT0SRK1MGOEU|      Twoapennything|  0|  0|  4|1342051200|          Nice Taffy|I got a wild hair...|\n",
      "| 13|B0009XLVG0| A327PCT23YH90|                  LT|  1|  1|  1|1339545600|My Cats Are Not F...|My cats have been...|\n",
      "| 14|B001GVISJM|A18ECVX2RJ7HUE| \"willie \"\"roadie\"\"\"|  2|  2|  4|1288915200|   fresh and greasy!|good flavor! thes...|\n",
      "| 17|B001GVISJM|A3KLWF6WQ5BNYO|      Erica Neathery|  0|  0|  2|1348099200|          poor taste|I love eating the...|\n",
      "| 27|B001GVISJM|A3RXAU2N8KV45G|              lady21|  0|  1|  1|1332633600|     Nasty No flavor|The candy is just...|\n",
      "| 28|B001GVISJM| AAAS38B98HMIK|        Heather Dube|  0|  1|  4|1331856000|Great Bargain for...|I was so glad Ama...|\n",
      "| 33|B001EO5QW8| AOVROBZ8BNTP7|           S. Potter| 19| 19|  4|1163376000|Best of the Insta...|McCann's Instant ...|\n",
      "| 34|B001EO5QW8|A3PMM0NFVEJGK9|\"Megan \"\"Bad at N...| 13| 13|  4|1166313600|        Good Instant|This is a good in...|\n",
      "| 36|B001EO5QW8|A2CI0RLADCRKPF|          T. J. Ryan|  3|  3|  4|1210464000|          satisfying|McCann's Instant ...|\n",
      "| 39|B001EO5QW8|A2GHZ2UTV2B0CD|         JERRY REITH|  0|  0|  4|1350777600|GOOD WAY TO START...|I WAS VISITING MY...|\n",
      "| 46|B001EO5QW8|A39Z97950MCTQE|         K. A. Freel|  0|  0|  3|1205193600|      Hearty Oatmeal|This seems a litt...|\n",
      "| 48|B001EO5QW8|A26AY1TFK8BQXQ|\"kbogo \"\"shoelove...|  1|  2|  3|1200096000|               Mushy|The flavors are g...|\n",
      "| 49|B001EO5QW8| ALOR97KTZTK1P|        knitty pants|  1|  2|  4|1191715200|Very good but nex...|I really like the...|\n",
      "| 50|B001EO5QW8|A276999Y6VRSCQ|                JMay|  0|  1|  3|1334016000|          Same stuff|This is the same ...|\n",
      "| 51|B001EO5QW8|A108P30XVUFKXY|           Roberto A|  0|  7|  1|1203379200|       Don't like it|This oatmeal is n...|\n",
      "| 53|B000G6RPMY| A9L6L5H9BPEBO|     Edwin C. Pauzer|  1|  1|  4|1348876800|You'll go nuts ov...|This wasn't in st...|\n",
      "| 54|B000G6RPMY| AQ9DWWYP2KJCQ|\"Roel Trevino \"\"p...|  0|  0|  3|1278028800|      not ass kickin|we're used to spi...|\n",
      "+---+----------+--------------+--------------------+---+---+---+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"_c6\"] < \"5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da1973",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1ebcc17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+---+---+---------+----------+--------------------+--------------------+\n",
      "|_c0|       _c1|           _c2|                 _c3|_c4|_c5|      _c6|       _c7|                 _c8|                 _c9|\n",
      "+---+----------+--------------+--------------------+---+---+---------+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|  1|  1|        1|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|  0|  0|        0|1346976000|   Not as Advertised|\"Product arrived ...|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|  1|  1|        1|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|  3|  3|        3|1307923200|      Cough Medicine|If you are lookin...|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|  0|  0|        0|1350777600|         Great taffy|Great taffy at a ...|\n",
      "|  6|B006K2ZZ7K| ADT0SRK1MGOEU|      Twoapennything|  0|  0|        0|1342051200|          Nice Taffy|I got a wild hair...|\n",
      "|  7|B006K2ZZ7K|A1SP2KVKFXXRU1|   David C. Sullivan|  0|  0|        0|1340150400|Great!  Just as g...|This saltwater ta...|\n",
      "|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|  0|  0|        0|1336003200|Wonderful, tasty ...|This taffy is so ...|\n",
      "|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|  1|  1|        1|1322006400|          Yay Barley|Right now I'm mos...|\n",
      "| 10|B00171APVA|A21BT40VZCCYT4|       Carol A. Reed|  0|  0|        0|1351209600|    Healthy Dog Food|This is a very he...|\n",
      "| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|  1|  1|        1|1107820800|The Best Hot Sauc...|I don't know if i...|\n",
      "| 12|B0009XLVG0|A2725IB4YY9JEB|\"A Poeng \"\"Sparky...|  4|  4|        4|1282867200|\"My cats LOVE thi...|One of my boys ne...|\n",
      "| 13|B0009XLVG0| A327PCT23YH90|                  LT|  1|  1|        1|1339545600|My Cats Are Not F...|My cats have been...|\n",
      "| 14|B001GVISJM|A18ECVX2RJ7HUE| \"willie \"\"roadie\"\"\"|  2|  2|        2|1288915200|   fresh and greasy!|good flavor! thes...|\n",
      "| 15|B001GVISJM|A2MUGFV2TDQ47K|\"Lynrie \"\"Oh HELL...|  4|  5|muy bueno|1268352000|Strawberry Twizzl...|The Strawberry Tw...|\n",
      "| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|  4|  5|muy bueno|1262044800|Lots of twizzlers...|My daughter loves...|\n",
      "| 17|B001GVISJM|A3KLWF6WQ5BNYO|      Erica Neathery|  0|  0|        0|1348099200|          poor taste|I love eating the...|\n",
      "| 18|B001GVISJM| AFKW14U97Z6QO|               Becca|  0|  0|        0|1345075200|            Love it!|I am very satisfi...|\n",
      "| 19|B001GVISJM|A2A9X58G2GTBLP|             Wolfee1|  0|  0|        0|1324598400|  GREAT SWEET CANDY!|Twizzlers, Strawb...|\n",
      "| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|  0|  0|        0|1318032000|Home delivered tw...|Candy was deliver...|\n",
      "+---+----------+--------------+--------------------+---+---+---------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "newDf = df.withColumn('_c6', regexp_replace('_c5', '5', 'muy bueno'))\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c3b3",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c0f9be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c9|\n",
      "+--------------------+\n",
      "|I have bought sev...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review_delmartian = df.filter(df[\"_c3\"] == \"delmartian\").select(\"_c9\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8471748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = sc.textFile(review_delmartian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4c3f046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o393.partitions.\n: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.FileInputFormat.getPathStrings(FileInputFormat.java:500)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:444)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1132)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1132)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:181)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:181)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:181)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:178)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m counts \u001b[38;5;241m=\u001b[39m \u001b[43mdf_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/rdd.py:1893\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, numPartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partitionFunc\u001b[38;5;241m=\u001b[39mportable_hash):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/rdd.py:2138\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2138\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   2141\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/rdd.py:2583\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   2582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/rdd.py:2937\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/usr/share/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/share/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o393.partitions.\n: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.FileInputFormat.getPathStrings(FileInputFormat.java:500)\n\tat org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:444)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1132)\n\tat org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1132)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:181)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:181)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:181)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:178)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "counts = df_rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1afe6822",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/hduser/.local/lib/python3.8/site-packages/IPython/utils/py3compat.py:55 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[1;32m      2\u001b[0m conf1 \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontador\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[3]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m contarPalabras \u001b[38;5;241m=\u001b[39m df_rdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m linea: linea\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcountByValue()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m palabra, contador \u001b[38;5;129;01min\u001b[39;00m contarPalabras\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m/usr/share/spark/python/pyspark/context.py:350\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    347\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    355\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/hduser/.local/lib/python3.8/site-packages/IPython/utils/py3compat.py:55 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf1 = SparkConf().setAppName(\"contador\").setMaster(\"local[3]\")\n",
    "sc = SparkContext(conf = conf1)\n",
    "contarPalabras = df_rdd.flatMap(lambda linea: linea.split(\" \")).countByValue()\n",
    "for palabra, contador in contarPalabras.items():\n",
    "    print(\"{} : {}\".format(palabra, contador))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
